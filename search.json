[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression and Other Stories: Python Implementation",
    "section": "",
    "text": "Introduction\nThis book is the Python implementation for Regression and Other Stories by Gelman, Hill, and Vehtari, using Bambi. The official code can be found from Vehtari at https://github.com/avehtari/ROS-Examples/."
  },
  {
    "objectID": "ros/ch10/kidiq.html#a-single-predictor",
    "href": "ros/ch10/kidiq.html#a-single-predictor",
    "title": "1  Regression and Other Stories: KidIQ",
    "section": "1.1 A single predictor",
    "text": "1.1.1 A single binary predictor\n\nmodel_1 = bmb.Model(\"kid_score ~ mom_hs\", data=kidiq)\nidata_1 = model_1.fit()\naz.summary(idata_1, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:04<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      77.552\n      2.130\n      73.627\n      81.545\n    \n    \n      mom_hs\n      11.791\n      2.371\n      7.309\n      16.157\n    \n    \n      kid_score_sigma\n      19.886\n      0.676\n      18.673\n      21.184\n    \n  \n\n\n\n\n\n\n1.1.2 A single continuous predictor\n\nmodel_2 = bmb.Model(\"kid_score ~ mom_iq\", data=kidiq)\nidata_2 = model_2.fit()\naz.summary(idata_2, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:06<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      25.646\n      5.921\n      15.310\n      37.783\n    \n    \n      mom_iq\n      0.612\n      0.059\n      0.504\n      0.727\n    \n    \n      kid_score_sigma\n      18.312\n      0.627\n      17.109\n      19.448\n    \n  \n\n\n\n\n\n\n1.1.3 Displaying a regression line as a function of one input variable\n\nstats_2 = az.summary(idata_2, kind=\"stats\")[\"mean\"]\nx = np.arange(kidiq[\"mom_iq\"].min(), kidiq[\"mom_iq\"].max())\ny_pred = stats_2[\"Intercept\"] + stats_2[\"mom_iq\"] * x\n\nfig, ax = plt.subplots()\nax.scatter(\"mom_iq\", \"kid_score\", data=kidiq, color=\"black\", s=10)\nax.set_xlabel(\"Mother IQ score\")\nax.set_ylabel(\"Child test score\")\nax.plot(x, y_pred, color=\"black\")"
  },
  {
    "objectID": "ros/ch10/kidiq.html#two-predictors",
    "href": "ros/ch10/kidiq.html#two-predictors",
    "title": "1  Regression and Other Stories: KidIQ",
    "section": "1.2 Two predictors",
    "text": "1.2.1 Linear regression\n\nmodel_3 = bmb.Model(\"kid_score ~ mom_hs + mom_iq\", data=kidiq)\nidata_3 = model_3.fit()\naz.summary(idata_3, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:13<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      25.678\n      5.946\n      14.861\n      37.316\n    \n    \n      mom_hs\n      5.950\n      2.201\n      1.996\n      10.288\n    \n    \n      mom_iq\n      0.564\n      0.061\n      0.449\n      0.676\n    \n    \n      kid_score_sigma\n      18.161\n      0.624\n      17.068\n      19.377"
  },
  {
    "objectID": "ros/ch10/kidiq.html#graphical-displays-of-data-and-fitted-models",
    "href": "ros/ch10/kidiq.html#graphical-displays-of-data-and-fitted-models",
    "title": "1  Regression and Other Stories: KidIQ",
    "section": "1.3 Graphical displays of data and fitted models",
    "text": "1.3.1 Two fitted regression lines – model with no interaction\n\nstats_3 = az.summary(idata_3, kind=\"stats\")[\"mean\"]\nx = np.arange(kidiq[\"mom_iq\"].min(), kidiq[\"mom_iq\"].max())\ny_pred_mom_hs_1 = stats_3[\"Intercept\"] + stats_3[\"mom_hs\"] + stats_3[\"mom_iq\"] * x\ny_pred_mom_hs_0 = stats_3[\"Intercept\"] + stats_3[\"mom_iq\"] * x\n\nfig, ax = plt.subplots()\nax.scatter(\"mom_iq\", \"kid_score\", data=kidiq.query(\"mom_hs == 1\"), color=\"black\", s=10)\nax.scatter(\"mom_iq\", \"kid_score\", data=kidiq.query(\"mom_hs == 0\"), color=\"gray\", s=10)\nax.plot(x, y_pred_mom_hs_1, color=\"black\")\nax.plot(x, y_pred_mom_hs_0, color=\"gray\")\n\n\n\n\n\n\n1.3.2 Two fitted regression lines – model with interaction\n\nmodel_4 = bmb.Model(\"kid_score ~ mom_hs + mom_iq + mom_hs:mom_iq\", data=kidiq)\nidata_4 = model_4.fit()\naz.summary(idata_4, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:32<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      -9.671\n      13.954\n      -35.952\n      16.048\n    \n    \n      mom_hs\n      49.183\n      15.520\n      21.813\n      79.798\n    \n    \n      mom_iq\n      0.949\n      0.151\n      0.665\n      1.227\n    \n    \n      mom_hs:mom_iq\n      -0.462\n      0.164\n      -0.764\n      -0.154\n    \n    \n      kid_score_sigma\n      18.000\n      0.617\n      16.815\n      19.118\n    \n  \n\n\n\n\n\nstats_4 = az.summary(idata_4, kind=\"stats\")[\"mean\"]\nx = np.arange(kidiq[\"mom_iq\"].min(), kidiq[\"mom_iq\"].max())\ny_pred_mom_hs_1 = (\n    stats_4[\"Intercept\"]\n    + stats_4[\"mom_hs\"]\n    + (stats_4[\"mom_hs:mom_iq\"] + stats_4[\"mom_iq\"]) * x\n)\ny_pred_mom_hs_0 = stats_4[\"Intercept\"] + stats_4[\"mom_iq\"] * x\n\nfig, ax = plt.subplots()\nax.scatter(\"mom_iq\", \"kid_score\", data=kidiq.query(\"mom_hs == 1\"), color=\"black\", s=10)\nax.scatter(\"mom_iq\", \"kid_score\", data=kidiq.query(\"mom_hs == 0\"), color=\"gray\", s=10)\nax.plot(x, y_pred_mom_hs_1, color=\"black\")\nax.plot(x, y_pred_mom_hs_0, color=\"gray\");"
  },
  {
    "objectID": "ros/ch10/kidiq.html#displaying-uncertainty-in-the-fitted-regression",
    "href": "ros/ch10/kidiq.html#displaying-uncertainty-in-the-fitted-regression",
    "title": "1  Regression and Other Stories: KidIQ",
    "section": "1.4 Displaying uncertainty in the fitted regression",
    "text": "1.4.1 A single continuous predictor\n\nindex = rng.choice(len(idata_2.posterior[\"draw\"].values), 10)\nintercept_sample = idata_2.posterior[\"Intercept\"][0, index].values\nslope_sample = idata_2.posterior[\"mom_iq\"][0, index].values\nx = np.arange(kidiq[\"mom_iq\"].min(), kidiq[\"mom_iq\"].max())\ny_pred = (\n    idata_2.posterior[\"Intercept\"].values.mean()\n    + idata_2.posterior[\"mom_iq\"].values.mean() * x\n)\n\nfig, ax = plt.subplots()\nax.scatter(\"mom_iq\", \"kid_score\", data=kidiq, color=\"black\", s=10)\nfor intercept, slope in zip(intercept_sample, slope_sample):\n    ax.plot(x, intercept + slope * x, color=\"gray\", alpha=0.5)\nax.plot(x, y_pred, color=\"black\")\n\nax.set_xlabel(\"Mother IQ score\")\nax.set_ylabel(\"Child test score\");\n\n\n\n\n\n\n1.4.2 Two predictors\n# TODO: add plot code here\n\n\n1.4.3 Center predictors to have zero mean\n\nkidiq[\"c_mom_hs\"] = kidiq[\"mom_hs\"] - kidiq[\"mom_hs\"].mean()\nkidiq[\"c_mom_iq\"] = kidiq[\"mom_iq\"] - kidiq[\"mom_iq\"].mean()\nmodel_4c = bmb.Model(\n    \"kid_score ~ c_mom_hs + c_mom_iq + c_mom_hs:c_mom_iq\",\n    data=kidiq,\n)\nidata_4c = model_4c.fit()\naz.summary(idata_4c)\n\n\n    \n        \n      \n      100.00% [8000/8000 00:07<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      87.640\n      0.912\n      85.894\n      89.346\n      0.014\n      0.010\n      4003.0\n      3038.0\n      1.0\n    \n    \n      c_mom_hs\n      2.755\n      2.368\n      -1.498\n      7.267\n      0.040\n      0.029\n      3415.0\n      2573.0\n      1.0\n    \n    \n      c_mom_iq\n      0.590\n      0.061\n      0.479\n      0.705\n      0.001\n      0.001\n      4810.0\n      3095.0\n      1.0\n    \n    \n      c_mom_hs:c_mom_iq\n      -0.485\n      0.159\n      -0.772\n      -0.175\n      0.002\n      0.002\n      4816.0\n      3545.0\n      1.0\n    \n    \n      kid_score_sigma\n      18.002\n      0.605\n      16.930\n      19.136\n      0.008\n      0.006\n      5254.0\n      3572.0\n      1.0\n    \n  \n\n\n\n\n\n\n1.4.4 Center predictors based on a reference point\n\nkidiq[\"c2_mom_hs\"] = kidiq[\"mom_hs\"] - 0.5\nkidiq[\"c2_mom_iq\"] = kidiq[\"mom_iq\"] - 100\nmodel_4c2 = bmb.Model(\n    \"kid_score ~ c2_mom_hs + c2_mom_iq + c2_mom_hs:c2_mom_iq\",\n    data=kidiq,\n)\nidata_4c2 = model_4c2.fit()\naz.summary(idata_4c2)\n\n\n    \n        \n      \n      100.00% [8000/8000 00:07<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      86.818\n      1.210\n      84.457\n      88.981\n      0.025\n      0.017\n      2387.0\n      2341.0\n      1.0\n    \n    \n      c2_mom_hs\n      2.883\n      2.386\n      -1.652\n      7.248\n      0.049\n      0.035\n      2381.0\n      2572.0\n      1.0\n    \n    \n      c2_mom_iq\n      0.726\n      0.082\n      0.582\n      0.888\n      0.002\n      0.001\n      2074.0\n      2479.0\n      1.0\n    \n    \n      c2_mom_hs:c2_mom_iq\n      -0.482\n      0.163\n      -0.799\n      -0.188\n      0.003\n      0.002\n      2172.0\n      2444.0\n      1.0\n    \n    \n      kid_score_sigma\n      18.000\n      0.619\n      16.896\n      19.269\n      0.010\n      0.007\n      3772.0\n      3164.0\n      1.0\n    \n  \n\n\n\n\n\n\n1.4.5 Center and scale predictors to have zero mean and sd=1/2\n\nkidiq[\"z_mom_hs\"] = (kidiq[\"mom_hs\"] - kidiq[\"mom_hs\"].mean()) / (\n    2 * kidiq[\"mom_hs\"].std()\n)\nkidiq[\"z_mom_iq\"] = (kidiq[\"mom_iq\"] - kidiq[\"mom_iq\"].mean()) / (\n    2 * kidiq[\"mom_iq\"].std()\n)\nmodel_4c = bmb.Model(\n    \"kid_score ~ z_mom_hs + z_mom_iq + z_mom_hs:z_mom_iq\",\n    data=kidiq,\n)\nidata_4c = model_4c.fit()\naz.summary(idata_4c)\n\n\n    \n        \n      \n      100.00% [8000/8000 00:07<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      87.648\n      0.937\n      85.868\n      89.388\n      0.012\n      0.009\n      6013.0\n      3314.0\n      1.0\n    \n    \n      z_mom_hs\n      2.299\n      2.018\n      -1.590\n      6.002\n      0.034\n      0.025\n      3598.0\n      3456.0\n      1.0\n    \n    \n      z_mom_iq\n      17.676\n      1.846\n      14.433\n      21.301\n      0.027\n      0.019\n      4519.0\n      3154.0\n      1.0\n    \n    \n      z_mom_hs:z_mom_iq\n      -12.069\n      3.996\n      -19.596\n      -4.583\n      0.064\n      0.047\n      3939.0\n      3062.0\n      1.0\n    \n    \n      kid_score_sigma\n      18.002\n      0.616\n      16.880\n      19.145\n      0.009\n      0.006\n      5097.0\n      3149.0\n      1.0\n    \n  \n\n\n\n\n\n\n1.4.6 Predict using working status of mother\n\nmodel_5 = bmb.Model(\"kid_score ~ C(mom_work)\", data=kidiq)\nidata_5 = model_5.fit()\naz.summary(idata_5, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:04<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      81.994\n      2.292\n      77.896\n      86.441\n    \n    \n      C(mom_work)[2]\n      3.866\n      3.090\n      -1.804\n      9.844\n    \n    \n      C(mom_work)[3]\n      11.504\n      3.561\n      4.844\n      18.084\n    \n    \n      C(mom_work)[4]\n      5.213\n      2.672\n      -0.025\n      9.851\n    \n    \n      kid_score_sigma\n      20.289\n      0.696\n      19.009\n      21.646"
  },
  {
    "objectID": "ros/ch10/height_and_weight.html#simulating-uncertainty-for-linear-predictors-and-predicted-values",
    "href": "ros/ch10/height_and_weight.html#simulating-uncertainty-for-linear-predictors-and-predicted-values",
    "title": "2  Regression and Other Stories: Height and weight",
    "section": "2.1 Simulating uncertainty for linear predictors and predicted values",
    "text": "2.1.1 Predict weight (in pounds) from height (in inches)\n\nmodel_1 = bmb.Model(\"weight ~ height\", data=earnings, dropna=True)\nidata_1 = model_1.fit()\naz.summary(idata_1, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:03<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      -172.922\n      11.899\n      -194.217\n      -149.535\n    \n    \n      height\n      4.944\n      0.179\n      4.597\n      5.263\n    \n    \n      weight_sigma\n      28.980\n      0.489\n      28.062\n      29.889\n    \n  \n\n\n\n\nPredict weight for 66 inches person\n\nstats_1 = az.summary(idata_1, kind=\"stats\")[\"mean\"]\npredicted_1 = stats_1[\"Intercept\"] + stats_1[\"height\"] * 66\nround(predicted_1, 1)\n\n153.4\n\n\nor\n\nnew = pd.DataFrame(dict(height=[66]))\nmodel_1.predict(idata_1, data=new)\npredicted_1 = idata_1.posterior[\"weight_mean\"].values.mean()\nround(predicted_1, 1)\n\n153.4\n\n\n\n\n2.1.2 Center heights\n\nearnings[\"c_height\"] = earnings[\"height\"] - 66\nmodel_2 = bmb.Model(\"weight ~ c_height\", data=earnings, dropna=True)\nidata_2 = model_2.fit()\naz.summary(idata_2, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:04<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      153.373\n      0.698\n      152.118\n      154.718\n    \n    \n      c_height\n      4.948\n      0.179\n      4.628\n      5.290\n    \n    \n      weight_sigma\n      28.961\n      0.486\n      28.057\n      29.855\n    \n  \n\n\n\n\n\n\n2.1.3 Point prediction\n\nnew = pd.DataFrame(dict(c_height=[4]))\nmodel_2.predict(idata_2, data=new)\npredicted_2 = idata_2.posterior[\"weight_mean\"].values.mean()\nround(predicted_2, 1)\n\n173.2\n\n\n\n\n2.1.4 Posterior simulations\nvariation coming from posterior uncertainty in the coefficients\n\nlinpred_2 = model_2.predict(idata_2, data=new, inplace=False)\nplt.hist(linpred_2.posterior[\"weight_mean\"].values.ravel(), color=\"gray\");\n\n\n\n\n\n\n2.1.5 Posterior predictive simulations\nvariation coming from posterior uncertainty in the coefficients and predictive uncertainty\n\npostpred_2 = model_2.predict(idata_2, data=new, kind=\"pps\", inplace=False)\nplt.hist(postpred_2.posterior_predictive[\"weight\"].values.ravel(), color=\"gray\");"
  },
  {
    "objectID": "ros/ch10/height_and_weight.html#indicator-variables",
    "href": "ros/ch10/height_and_weight.html#indicator-variables",
    "title": "2  Regression and Other Stories: Height and weight",
    "section": "2.2 Indicator variables",
    "text": "2.2.1 Predict weight (in pounds) from height (in inches)\n\nnew = pd.DataFrame(dict(height=[66]))\npred = model_1.predict(idata_1, data=new, kind=\"pps\", inplace=False)\npred_mean = pred.posterior_predictive[\"weight\"].values.mean()\npred_sd = pred.posterior_predictive[\"weight\"].values.std()\nprint(\n    f\"Predicted weight for a 66-inch-tall person is {round(pred_mean)} pounds with a sd of {round(pred_sd)}\"\n)\n\nPredicted weight for a 66-inch-tall person is 153 pounds with a sd of 29\n\n\n\n\n2.2.2 Including a binary variable in a regression\n\nmodel_3 = bmb.Model(\"weight ~ c_height + male\", data=earnings, dropna=True)\nidata_3 = model_3.fit()\naz.summary(idata_3, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:06<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      149.530\n      0.951\n      147.783\n      151.342\n    \n    \n      c_height\n      3.881\n      0.253\n      3.418\n      4.374\n    \n    \n      male\n      11.867\n      2.028\n      8.027\n      15.691\n    \n    \n      weight_sigma\n      28.690\n      0.487\n      27.762\n      29.588\n    \n  \n\n\n\n\n\nnew = pd.DataFrame(dict(c_height=[4], male=[0]))\npred = model_3.predict(idata_3, data=new, kind=\"pps\", inplace=False)\npred_mean = pred.posterior_predictive[\"weight\"].values.mean()\npred_sd = pred.posterior_predictive[\"weight\"].values.std()\nprint(\n    f\"Predicted weight for a 70-inch-tall female is {round(pred_mean)} pounds with a sd of {round(pred_sd)}\"\n)\n\nPredicted weight for a 70-inch-tall female is 165 pounds with a sd of 29\n\n\n\n\n2.2.3 Using indicator variables for multiple levels of a categorical predictor\nInclude ethnicity in the regression as a factor\n\nmodel_4 = bmb.Model(\n    \"weight ~ c_height + male + C(ethnicity)\", data=earnings, dropna=True\n)\nidata_4 = model_4.fit()\naz.summary(idata_4, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:04<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      154.356\n      2.208\n      150.135\n      158.402\n    \n    \n      c_height\n      3.851\n      0.252\n      3.380\n      4.308\n    \n    \n      male\n      12.112\n      1.988\n      8.309\n      15.860\n    \n    \n      C(ethnicity)[Hispanic]\n      -6.132\n      3.546\n      -12.492\n      0.720\n    \n    \n      C(ethnicity)[Other]\n      -12.239\n      5.125\n      -20.977\n      -1.720\n    \n    \n      C(ethnicity)[White]\n      -5.208\n      2.261\n      -9.204\n      -0.890\n    \n    \n      weight_sigma\n      28.641\n      0.467\n      27.777\n      29.504\n    \n  \n\n\n\n\n\n\n2.2.4 Choose the baseline category by setting the levels\n\nearnings[\"eth\"] = pd.Categorical(\n    earnings[\"ethnicity\"],\n    categories=[\"White\", \"Black\", \"Hispanic\", \"Other\"],\n    ordered=True,\n)\nmodel_5 = bmb.Model(\"weight ~ c_height + male + eth\", data=earnings, dropna=True)\nidata_5 = model_5.fit()\naz.summary(idata_5, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:04<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      149.133\n      0.971\n      147.309\n      150.953\n    \n    \n      c_height\n      3.857\n      0.252\n      3.388\n      4.334\n    \n    \n      male\n      12.077\n      1.995\n      8.223\n      15.741\n    \n    \n      eth[Black]\n      5.226\n      2.288\n      1.105\n      9.759\n    \n    \n      eth[Hispanic]\n      -0.924\n      2.954\n      -6.332\n      4.814\n    \n    \n      eth[Other]\n      -7.173\n      4.819\n      -15.649\n      2.175\n    \n    \n      weight_sigma\n      28.656\n      0.477\n      27.747\n      29.518\n    \n  \n\n\n\n\n\n\n2.2.5 Alternatively create indicators for the four ethnic groups directly:\nearnings[\"eth_white\"] = earnings[\"ethnicity\"] == \"White\"\nearnings[\"eth_black\"] = earnings[\"ethnicity\"] == \"Black\"\nearnings[\"eth_hispanic\"] = earnings[\"ethnicity\"] == \"Hispanic\"\nearnings[\"eth_other\"] = earnings[\"ethnicity\"] == \"Other\"\n\nmodel_6 = bmb.Model(\n    \"weight ~ c_height + male + eth_black + eth_hispanic + eth_other\",\n    data=earnings,\n    dropna=True,\n)\nidata_6 = model_6.fit()\naz.summary(idata_6, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:15<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      149.125\n      0.979\n      147.387\n      151.050\n    \n    \n      c_height\n      3.851\n      0.252\n      3.390\n      4.330\n    \n    \n      male\n      12.120\n      1.976\n      8.395\n      15.663\n    \n    \n      eth_black\n      5.191\n      2.225\n      1.037\n      9.428\n    \n    \n      eth_hispanic\n      -0.970\n      2.937\n      -6.516\n      4.384\n    \n    \n      eth_other\n      -7.013\n      4.819\n      -16.082\n      2.245\n    \n    \n      weight_sigma\n      28.656\n      0.474\n      27.810\n      29.576"
  },
  {
    "objectID": "ros/ch10/beauty.html#do-more-beautiful-profs-get-higher-evaluations",
    "href": "ros/ch10/beauty.html#do-more-beautiful-profs-get-higher-evaluations",
    "title": "3  Regression and Other Stories: Beauty and Teaching Quality",
    "section": "3.1 Do more beautiful profs get higher evaluations?",
    "text": "3.1.1 Make a scatterplot of data\n\nfig, ax = plt.subplots()\nax.scatter(\"beauty\", \"eval\", data=beauty, color=\"black\", s=10, alpha=0.7);\n\n\n\n\n\n\n3.1.2 Fit a linear regression\n\nmodel_1 = bmb.Model(\"eval ~ beauty\", data=beauty)\nidata_1 = model_1.fit()\naz.summary(idata_1, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:03<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      4.010\n      0.026\n      3.964\n      4.060\n    \n    \n      beauty\n      0.133\n      0.032\n      0.070\n      0.189\n    \n    \n      eval_sigma\n      0.547\n      0.019\n      0.514\n      0.584\n    \n  \n\n\n\n\n\nstats_1 = az.summary(idata_1, kind=\"stats\")[\"mean\"]\nx = np.linspace(beauty[\"beauty\"].min(), beauty[\"beauty\"].max(), 10)\ny_pred = stats_1[\"Intercept\"] + stats_1[\"beauty\"] * x\n\nfig, ax = plt.subplots()\nax.scatter(\"beauty\", \"eval\", data=beauty, color=\"black\", s=10)\nax.set_xlabel(\"Beauty\")\nax.set_ylabel(\"Average teaching evaluation\")\nax.plot(x, y_pred, color=\"black\")\nax.plot(x, y_pred + stats_1[\"eval_sigma\"], color=\"black\", linestyle=\"--\")\nax.plot(x, y_pred - stats_1[\"eval_sigma\"], color=\"black\", linestyle=\"--\");"
  },
  {
    "objectID": "ros/ch10/beauty.html#do-things-differ-for-male-and-female-profs",
    "href": "ros/ch10/beauty.html#do-things-differ-for-male-and-female-profs",
    "title": "3  Regression and Other Stories: Beauty and Teaching Quality",
    "section": "3.2 Do things differ for male and female profs?",
    "text": "3.2.1 Parallel regression lines\n\nmodel_2 = bmb.Model(\"eval ~ beauty + female\", data=beauty)\nidata_2 = model_2.fit()\naz.summary(idata_2, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:06<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      4.094\n      0.033\n      4.032\n      4.154\n    \n    \n      beauty\n      0.148\n      0.032\n      0.087\n      0.209\n    \n    \n      female\n      -0.198\n      0.052\n      -0.295\n      -0.104\n    \n    \n      eval_sigma\n      0.538\n      0.018\n      0.506\n      0.573\n    \n  \n\n\n\n\n\n\n3.2.2 Make two subplots\n\nstats_2 = az.summary(idata_2, kind=\"stats\")[\"mean\"]\nx = np.linspace(beauty[\"beauty\"].min(), beauty[\"beauty\"].max(), 10)\ny_pred_male = stats_2[\"Intercept\"] + stats_2[\"beauty\"] * x\ny_pred_female = stats_2[\"Intercept\"] + stats_2[\"female\"] + stats_2[\"beauty\"] * x\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 9))\n\n# Men\nax1.scatter(\"beauty\", \"eval\", data=beauty.query(\"female == 0\"), color=\"grey\", s=10)\nax1.set_title(\"Men\")\nax1.set_xlabel(\"Beauty\")\nax1.set_ylabel(\"Average teaching evaluation\")\nax1.plot(x, y_pred_male, color=\"grey\")\n\n# Women\nax2.scatter(\"beauty\", \"eval\", data=beauty.query(\"female == 1\"), color=\"black\", s=10)\nax2.set_title(\"Women\")\nax2.set_xlabel(\"Beauty\")\nax2.set_ylabel(\"Average teaching evaluation\")\nax2.plot(x, y_pred_female, color=\"black\")\n\n# Both sexes\nax3.scatter(\"beauty\", \"eval\", data=beauty.query(\"female == 1\"), color=\"black\", s=10)\nax3.scatter(\"beauty\", \"eval\", data=beauty.query(\"female == 0\"), color=\"grey\", s=10)\nax3.set_xlabel(\"Beauty\")\nax3.set_ylabel(\"Average teaching evaluation\")\nax3.plot(x, y_pred_female, color=\"black\")\nax3.plot(x, y_pred_male, color=\"grey\")\n\nax4.remove()\nplt.tight_layout();"
  },
  {
    "objectID": "ros/ch10/beauty.html#do-things-differ-for-male-and-female-profs-1",
    "href": "ros/ch10/beauty.html#do-things-differ-for-male-and-female-profs-1",
    "title": "3  Regression and Other Stories: Beauty and Teaching Quality",
    "section": "3.3 Do things differ for male and female profs?",
    "text": "3.3.1 Non-parallel regression lines\n\nmodel_3 = bmb.Model(\"eval ~ beauty + female + beauty*female\", data=beauty)\nidata_3 = model_3.fit()\naz.summary(idata_3, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:10<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      4.104\n      0.033\n      4.041\n      4.165\n    \n    \n      beauty\n      0.200\n      0.043\n      0.124\n      0.284\n    \n    \n      female\n      -0.205\n      0.052\n      -0.303\n      -0.108\n    \n    \n      beauty:female\n      -0.113\n      0.063\n      -0.231\n      0.002\n    \n    \n      eval_sigma\n      0.537\n      0.017\n      0.502\n      0.568\n    \n  \n\n\n\n\n\nstats_3 = az.summary(idata_3, kind=\"stats\")[\"mean\"]\nx = np.linspace(beauty[\"beauty\"].min(), beauty[\"beauty\"].max(), 10)\ny_pred_male_interaction = stats_3[\"Intercept\"] + stats_3[\"beauty\"] * x\ny_pred_female_interaction = (\n    stats_3[\"Intercept\"]\n    + stats_3[\"female\"] * 1\n    + stats_3[\"beauty\"] * x\n    + stats_3[\"beauty:female\"] * x * 1\n)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Men\nax1.scatter(\"beauty\", \"eval\", data=beauty.query(\"female == 0\"), color=\"black\", s=10)\nax1.set_title(\"Men\")\nax1.set_xlabel(\"Beauty\")\nax1.set_ylabel(\"Average teaching evaluation\")\nax1.plot(x, y_pred_male, color=\"black\", alpha=0.3)\nax1.plot(x, y_pred_male_interaction, color=\"black\")\n\n# Women\nax2.scatter(\"beauty\", \"eval\", data=beauty.query(\"female == 1\"), color=\"black\", s=10)\nax2.set_title(\"Women\")\nax2.set_xlabel(\"Beauty\")\nax2.set_ylabel(\"Average teaching evaluation\")\nax2.plot(x, y_pred_female, color=\"black\", alpha=0.3)\nax2.plot(x, y_pred_female_interaction, color=\"black\")\n\nplt.tight_layout();"
  },
  {
    "objectID": "ros/ch10/beauty.html#more-models",
    "href": "ros/ch10/beauty.html#more-models",
    "title": "3  Regression and Other Stories: Beauty and Teaching Quality",
    "section": "3.4 More models",
    "text": "3.4.1 Add age\n\nmodel_4 = bmb.Model(\"eval ~ beauty + female + age\", data=beauty)\nidata_4 = model_4.fit()\naz.summary(idata_4, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:10<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      4.223\n      0.143\n      3.965\n      4.503\n    \n    \n      beauty\n      0.140\n      0.034\n      0.078\n      0.205\n    \n    \n      female\n      -0.210\n      0.053\n      -0.308\n      -0.112\n    \n    \n      age\n      -0.003\n      0.003\n      -0.008\n      0.003\n    \n    \n      eval_sigma\n      0.539\n      0.018\n      0.504\n      0.572\n    \n  \n\n\n\n\n\n\n3.4.2 Add minority\n\nmodel_5 = bmb.Model(\"eval ~ beauty + female + minority\", data=beauty)\nidata_5 = model_5.fit()\naz.summary(idata_5, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:06<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      4.105\n      0.035\n      4.038\n      4.168\n    \n    \n      beauty\n      0.150\n      0.033\n      0.088\n      0.210\n    \n    \n      female\n      -0.189\n      0.053\n      -0.287\n      -0.093\n    \n    \n      minority\n      -0.103\n      0.073\n      -0.239\n      0.032\n    \n    \n      eval_sigma\n      0.538\n      0.018\n      0.505\n      0.573\n    \n  \n\n\n\n\n\n\n3.4.3 Add nonenglish\n\nmodel_6 = bmb.Model(\"eval ~ beauty + female + nonenglish\", data=beauty)\nidata_6 = model_6.fit()\naz.summary(idata_6, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:05<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      4.115\n      0.034\n      4.051\n      4.179\n    \n    \n      beauty\n      0.150\n      0.032\n      0.092\n      0.213\n    \n    \n      female\n      -0.198\n      0.052\n      -0.296\n      -0.102\n    \n    \n      nonenglish\n      -0.334\n      0.103\n      -0.537\n      -0.151\n    \n    \n      eval_sigma\n      0.533\n      0.018\n      0.501\n      0.569\n    \n  \n\n\n\n\n\n\n3.4.4 Add nonenglish and lower\n\nmodel_7 = bmb.Model(\"eval ~ beauty + female + nonenglish + lower\", data=beauty)\nidata_7 = model_7.fit()\naz.summary(idata_7, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:05<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      4.078\n      0.039\n      4.005\n      4.152\n    \n    \n      beauty\n      0.147\n      0.032\n      0.089\n      0.210\n    \n    \n      female\n      -0.191\n      0.052\n      -0.290\n      -0.097\n    \n    \n      nonenglish\n      -0.307\n      0.105\n      -0.488\n      -0.102\n    \n    \n      lower\n      0.094\n      0.053\n      -0.003\n      0.192\n    \n    \n      eval_sigma\n      0.532\n      0.018\n      0.500\n      0.565\n    \n  \n\n\n\n\n## Simple model with course indicators\n### Include course indicators in a regression\n\nmodel_8 = bmb.Model(\"eval ~ beauty + C(course_id)\", data=beauty)\nidata_8 = model_8.fit()\naz.summary(idata_8, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:04<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      4.031\n      0.031\n      3.974\n      4.091\n    \n    \n      beauty\n      0.136\n      0.033\n      0.074\n      0.200\n    \n    \n      C(course_id)[1]\n      0.364\n      0.239\n      -0.071\n      0.821\n    \n    \n      C(course_id)[2]\n      0.419\n      0.369\n      -0.268\n      1.106\n    \n    \n      C(course_id)[3]\n      -0.175\n      0.189\n      -0.527\n      0.186\n    \n    \n      C(course_id)[4]\n      -0.201\n      0.122\n      -0.436\n      0.021\n    \n    \n      C(course_id)[5]\n      0.017\n      0.254\n      -0.471\n      0.467\n    \n    \n      C(course_id)[6]\n      -0.126\n      0.211\n      -0.546\n      0.253\n    \n    \n      C(course_id)[7]\n      -0.325\n      0.272\n      -0.819\n      0.212\n    \n    \n      C(course_id)[8]\n      -0.143\n      0.378\n      -0.848\n      0.568\n    \n    \n      C(course_id)[9]\n      -0.427\n      0.187\n      -0.761\n      -0.066\n    \n    \n      C(course_id)[10]\n      0.421\n      0.234\n      -0.026\n      0.843\n    \n    \n      C(course_id)[11]\n      -0.071\n      0.364\n      -0.707\n      0.656\n    \n    \n      C(course_id)[12]\n      0.018\n      0.303\n      -0.556\n      0.572\n    \n    \n      C(course_id)[13]\n      -0.082\n      0.305\n      -0.643\n      0.499\n    \n    \n      C(course_id)[14]\n      -0.512\n      0.308\n      -1.057\n      0.105\n    \n    \n      C(course_id)[15]\n      -1.434\n      0.375\n      -2.093\n      -0.693\n    \n    \n      C(course_id)[16]\n      0.176\n      0.263\n      -0.313\n      0.690\n    \n    \n      C(course_id)[17]\n      0.336\n      0.202\n      -0.041\n      0.723\n    \n    \n      C(course_id)[18]\n      0.267\n      0.261\n      -0.222\n      0.754\n    \n    \n      C(course_id)[19]\n      -0.309\n      0.220\n      -0.715\n      0.098\n    \n    \n      C(course_id)[20]\n      0.456\n      0.244\n      0.008\n      0.922\n    \n    \n      C(course_id)[21]\n      -0.391\n      0.144\n      -0.685\n      -0.131\n    \n    \n      C(course_id)[22]\n      -0.286\n      0.162\n      -0.602\n      0.017\n    \n    \n      C(course_id)[23]\n      0.372\n      0.236\n      -0.060\n      0.800\n    \n    \n      C(course_id)[24]\n      -0.240\n      0.306\n      -0.823\n      0.333\n    \n    \n      C(course_id)[25]\n      -0.138\n      0.304\n      -0.687\n      0.456\n    \n    \n      C(course_id)[26]\n      0.234\n      0.307\n      -0.330\n      0.807\n    \n    \n      C(course_id)[27]\n      0.132\n      0.369\n      -0.527\n      0.851\n    \n    \n      C(course_id)[28]\n      0.431\n      0.262\n      -0.088\n      0.909\n    \n    \n      C(course_id)[29]\n      -0.089\n      0.373\n      -0.784\n      0.597\n    \n    \n      C(course_id)[30]\n      0.300\n      0.191\n      -0.045\n      0.659\n    \n    \n      eval_sigma\n      0.526\n      0.018\n      0.493\n      0.558"
  },
  {
    "objectID": "ros/ch10/congress.html#regression-predicting-1988-from-1986",
    "href": "ros/ch10/congress.html#regression-predicting-1988-from-1986",
    "title": "4  Regression and Other Stories: Congress",
    "section": "4.1 Regression predicting 1988 from 1986",
    "text": "data88 = pd.DataFrame(\n    dict(\n        vote=congress[\"v88_adj\"],\n        past_vote=congress[\"v86_adj\"],\n        inc=congress[\"inc88\"],\n    )\n)\n\nmodel_88 = bmb.Model(\"vote ~ past_vote + inc\", data=data88)\nidata_88 = model_88.fit()\naz.summary(idata_88, kind=\"stats\")\n\n\n    \n        \n      \n      100.00% [8000/8000 00:09<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      0.238\n      0.017\n      0.204\n      0.269\n    \n    \n      past_vote\n      0.520\n      0.033\n      0.459\n      0.582\n    \n    \n      inc\n      0.097\n      0.007\n      0.084\n      0.109\n    \n    \n      vote_sigma\n      0.067\n      0.002\n      0.063\n      0.072"
  },
  {
    "objectID": "ros/ch10/congress.html#simulation-for-inferences-and-predictions-of-new-data-points",
    "href": "ros/ch10/congress.html#simulation-for-inferences-and-predictions-of-new-data-points",
    "title": "4  Regression and Other Stories: Congress",
    "section": "4.2 Simulation for inferences and predictions of new data points",
    "text": "4.2.1 Predict from 1988 to 1990\ndata90 = pd.DataFrame(\n    dict(\n        past_vote=congress[\"v88_adj\"],\n        inc=congress[\"inc90\"],\n    )\n)\n\n\n4.2.2 Simulate predictive simulations of the vector of new outcomes\npred90 = model_88.predict(idata_88, data=data90, kind=\"pps\", inplace=False)\n\n\n4.2.3 Simulate the number of elections predicted to be won by the Democrats in 1990\ndems_pred = pred90.posterior_predictive[\"vote\"].values.reshape(-1, 435)\ndems_pred = (dems_pred > 0.5).sum(axis=1)\n\n\n4.2.4 Our posterior mean and sd of how many districts the Dems will win\n\nprint(f\"mean: {dems_pred.mean():.1f}, std: {dems_pred.std():.1f}\")\n\nmean: 260.0, std: 2.5\n\n\n\n\n4.2.5 Histogram of how many districts the Dems will win\n\nplt.hist(dems_pred, color=\"gray\");"
  },
  {
    "objectID": "ros/ch10/congress.html#graphs",
    "href": "ros/ch10/congress.html#graphs",
    "title": "4  Regression and Other Stories: Congress",
    "section": "4.3 Graphs",
    "text": "v88_hist = np.where(\n    congress[\"v88\"] < 0.1,\n    0.0001,\n    np.where(congress[\"v88\"] > 0.9, 0.9999, congress[\"v88\"]),\n)\n\nfig, ax = plt.subplots()\nax.hist(v88_hist, color=\"gray\", bins=np.arange(0, 1.05, 0.05))\nax.set_xlabel(\"Democratic share of the two-party vote\")\nax.set_title(\"Congressional elections in 1988\");\n\n\n\n\n\ndef jitt(vote):\n    n = len(vote)\n    return np.where(\n        vote < 0.1,\n        rng.uniform(0.01, 0.04, n),\n        np.where(vote > 0.9, rng.uniform(0.96, 0.99, n), vote),\n    )\n\n\nj_v86 = jitt(congress[\"v86\"])\nj_v88 = jitt(congress[\"v88\"])\n\nfig, ax = plt.subplots()\nax.scatter(\n    j_v86[congress[\"inc88\"] == 0],\n    j_v88[congress[\"inc88\"] == 0],\n    marker=\"*\",\n    c=\"black\",\n    s=20,\n)\nax.scatter(\n    j_v86[congress[\"inc88\"] == 1],\n    j_v88[congress[\"inc88\"] == 1],\n    marker=\"x\",\n    c=\"black\",\n    s=20,\n)\nax.scatter(\n    j_v86[congress[\"inc88\"] == -1],\n    j_v88[congress[\"inc88\"] == -1],\n    marker=\"o\",\n    c=\"black\",\n    s=20,\n)\nax.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n\nax.set_title(\"Raw data\")\nax.set_xlabel(\"Democratic vote share in 1986\")\nax.set_ylabel(\"Democratic vote share in 1988\")\n\nText(0, 0.5, 'Democratic vote share in 1988')\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\nax.set_xlabel(\"Democratic vote share in 1986\")\nax.set_ylabel(\"Democratic vote share in 1988\")\nax.scatter(\n    congress[\"v86_adj\"][congress[\"inc88\"] == 0],\n    congress[\"v88_adj\"][congress[\"inc88\"] == 0],\n    marker=\"*\",\n    c=\"black\",\n    s=20,\n)\nax.scatter(\n    congress[\"v86_adj\"][congress[\"inc88\"] == 1],\n    congress[\"v88_adj\"][congress[\"inc88\"] == 1],\n    marker=\"x\",\n    c=\"black\",\n    s=20,\n)\nax.scatter(\n    congress[\"v86_adj\"][congress[\"inc88\"] == -1],\n    congress[\"v88_adj\"][congress[\"inc88\"] == -1],\n    marker=\"o\",\n    c=\"black\",\n    s=20,\n)\nax.set_title(\"Adjusted data\")\n\nText(0.5, 1.0, 'Adjusted data')"
  },
  {
    "objectID": "ros/ch10/nes_linear.html",
    "href": "ros/ch10/nes_linear.html",
    "title": "5  Regression and Other Stories: National election study",
    "section": "",
    "text": "from pathlib import Path\n\nimport arviz as az\nimport bambi as bmb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nDATA_PATH = Path(\"../../data\")\n\n5.0.1 Load packages\nrng = np.random.default_rng(seed=12)\naz.style.use(\"arviz-white\")\n\n\n5.0.2 Load data\n\nnes = pd.read_csv(DATA_PATH / \"nes.txt\", delimiter=\" \")\nnes.head()\n\n\n\n\n  \n    \n      \n      year\n      resid\n      weight1\n      weight2\n      weight3\n      age\n      gender\n      race\n      educ1\n      urban\n      ...\n      parent_party\n      white\n      year_new\n      income_new\n      age_new\n      vote.1\n      age_discrete\n      race_adj\n      dvote\n      rvote\n    \n  \n  \n    \n      536\n      1952\n      1\n      1.0\n      1.0\n      1.0\n      25\n      2\n      1\n      2\n      2.0\n      ...\n      2.0\n      1\n      1\n      1\n      -2.052455\n      1.0\n      1\n      1.0\n      0.0\n      1.0\n    \n    \n      537\n      1952\n      2\n      1.0\n      1.0\n      1.0\n      33\n      2\n      1\n      1\n      2.0\n      ...\n      0.0\n      1\n      1\n      1\n      -1.252455\n      1.0\n      2\n      1.0\n      1.0\n      0.0\n    \n    \n      538\n      1952\n      3\n      1.0\n      1.0\n      1.0\n      26\n      2\n      1\n      2\n      2.0\n      ...\n      -2.0\n      1\n      1\n      0\n      -1.952455\n      1.0\n      1\n      1.0\n      0.0\n      1.0\n    \n    \n      539\n      1952\n      4\n      1.0\n      1.0\n      1.0\n      63\n      1\n      1\n      2\n      2.0\n      ...\n      NaN\n      1\n      1\n      0\n      1.747545\n      1.0\n      3\n      1.0\n      0.0\n      1.0\n    \n    \n      540\n      1952\n      5\n      1.0\n      1.0\n      1.0\n      66\n      2\n      1\n      2\n      2.0\n      ...\n      -2.0\n      1\n      1\n      -2\n      2.047545\n      1.0\n      4\n      1.0\n      0.0\n      1.0\n    \n  \n\n5 rows × 70 columns\n\n\n\n\n\n5.0.3 Partyid model to illustrate repeated model use (secret weapon)\n\ndef regress_year(year, data=nes):\n    this_year = data[data[\"year\"] == year]\n    model = bmb.Model(\n        \"partyid7 ~ real_ideo + race_adj + C(age_discrete) + educ1 + female + income\",\n        data=this_year,\n        dropna=True,\n    )\n    idata = model.fit()\n    stats = az.summary(idata, kind=\"stats\")\n    return (year, stats)\n\n\nyears = np.arange(1972, 2004, 4)\nsummary = [regress_year(year) for year in years]\n\n\n    \n        \n      \n      100.00% [8000/8000 00:04<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n    \n        \n      \n      100.00% [8000/8000 00:04<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n    \n        \n      \n      100.00% [8000/8000 00:04<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n    \n        \n      \n      100.00% [8000/8000 00:04<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n    \n        \n      \n      100.00% [8000/8000 00:04<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n    \n        \n      \n      100.00% [8000/8000 00:04<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n    \n        \n      \n      100.00% [8000/8000 00:05<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n    \n        \n      \n      100.00% [8000/8000 08:02<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n5.0.4 plot\n\ncoefs = [\n    \"Intercept\",\n    \"real_ideo\",\n    \"race_adj\",\n    \"C(age_discrete)[2]\",\n    \"C(age_discrete)[3]\",\n    \"C(age_discrete)[4]\",\n    \"educ1\",\n    \"female\",\n    \"income\",\n]\n\ncoef_names = [\n    \"Intercept\",\n    \"Ideology\",\n    \"Black\",\n    \"Age_30_44\",\n    \"Age_45_64\",\n    \"Age_65_up\",\n    \"Education\",\n    \"female\",\n    \"income\",\n]\n\nfig, axes = plt.subplots(2, 5, figsize=(15, 9))\n\nfor ax, coef in zip(axes.flat, coefs):\n    coef_each_year = [summary[i][1].loc[coef, \"mean\"] for i in range(len(summary))]\n    coef_sd_each_year = [summary[i][1].loc[coef, \"sd\"] for i in range(len(summary))]\n    ax.scatter(years, coef_each_year, color=\"black\")\n    ax.errorbar(\n        years, coef_each_year, yerr=coef_sd_each_year, fmt=\"none\", color=\"black\"\n    )\n    ax.axhline(0, linestyle=\"--\", color=\"gray\")\n    ax.set_title(coef_names[coefs.index(coef)])\n\naxes.flat[-1].remove()"
  }
]